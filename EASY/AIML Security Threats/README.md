# Introduction
Q. I'm ready to learn about AI/ML security threats!
--> no answer needed.

## The Building Blocks of AI
Q. What category of machine learning combines both labelled and unlabelled data?
--> semi-supervised learning

Q. What is the first layer in a neural network that handles incoming raw data?
--> input layer

Q. Which learning method does not require human-labeled data and can extract features from raw, unstructured input?
--> deep learning

Q. What are the weighted connections between nodes in a neural network meant to simulate in the human brain?
--> synapses

KEY POINTS:
Artificial Intelligence (AI) is a broad field focused on creating machines that can perform tasks requiring human-like intelligence.

Machine Learning (ML) is a subfield of AI where computers learn from data without being explicitly programmed. It follows a structured lifecycle and uses algorithms like supervised, unsupervised, semi-supervised, and reinforcement learning.

Neural Networks are a type of ML model inspired by the human brain, with layers of interconnected nodes (neurons) that process data.

Deep Learning (DL) is a more advanced form of ML that uses neural networks with multiple hidden layers. A key advantage is its ability to learn from large, unlabeled datasets without human intervention, making it a "scalable" form of ML. The recent growth of DL is due to the availability of massive amounts of digital data.

## LLMs (Large Language Model)
Q. What type of AI model enabled major advancements in ChatGPT and similar tools?
--> Large Language Models

Q. What is the first training stage where an LLM processes massive amounts of data?
--> pre-training

Q. What type of neural network introduced by Google in 2017 powers modern LLMs?
--> transformer

KEY POINTS:
Large Language Models (LLMs) are a type of Deep Learning model that can process and generate text by predicting the next word in a sequence.

Pre-training: LLMs are initially trained on massive, unlabelled text datasets (like the entire internet), using billions of parameters to understand and generate human-like language. This is where they learn grammar, facts, and reasoning.

Transformer Neural Networks revolutionized LLMs by allowing them to process text in parallel, rather than word by word. This architecture, introduced in a 2017 paper, uses an "attention" mechanism to understand context and the relationships between words.

Reinforcement Learning from Human Feedback (RLHF): After pre-training, human feedback is used to fine-tune the model, adjusting its parameters to make responses more helpful and less problematic.

Generative AI: LLMs power Generative AI, a broader category that creates original content, including text, images, and music.

The Big Picture: AI is the overarching field, ML is a subfield, DL is a specialized branch of ML, and LLMs are an advanced form of DL, specifically enabled by transformer neural networks.
